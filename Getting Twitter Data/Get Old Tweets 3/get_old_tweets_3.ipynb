{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import GetOldTweets3 as got\n",
    "import time\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "import re, string\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used to normalize raw tweets\n",
    "def clean_data(tweet_tokens, stop_words = ()):\n",
    "\n",
    "    cleaned_tokens = []\n",
    "\n",
    "    for token, tag in pos_tag(tweet_tokens):\n",
    "        # removing unwanted symbols and patterns from tokens using regular expressions\n",
    "        token = re.sub(\"http[s]?://+(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\",\"\", token)\n",
    "        token = re.sub(\".(.co/o).\",\"\", token)\n",
    "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
    "        token = re.sub(\"\\w*\\d\\w*\",\"\",token)\n",
    "        token = re.sub('[‘’“”…#–/]', '', token)\n",
    "#         token = re.sub(\"http[s]?\",\"\",token)\n",
    "        \n",
    "        # assigning new pos tags for WordNetLemmatizer() function\n",
    "        if tag.startswith(\"NN\"):\n",
    "            pos = \"n\"\n",
    "        elif tag.startswith(\"VB\"):\n",
    "            pos = \"v\"\n",
    "        else:\n",
    "            pos = \"a\"\n",
    "            \n",
    "        # lemmatizing tokens (running=run)\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        token = lemmatizer.lemmatize(token, pos)\n",
    "        \n",
    "        # dropping puncuation and stop words\n",
    "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
    "            cleaned_tokens.append(token.lower())\n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting to MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = \"mongodb://localhost:27017\"\n",
    "client = pymongo.MongoClient(conn)\n",
    "db = client.twitter_db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets(search_term, year):\n",
    "    months = [\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",]\n",
    "    all_tweets = []\n",
    "    count = 0\n",
    "\n",
    "    for month in months:\n",
    "        # setting parameters\n",
    "        tweetCriteria = got.manager.TweetCriteria() \\\n",
    "            .setQuerySearch(search_term)\\\n",
    "            .setSince(f\"{year}-{month}-01\")\\\n",
    "            .setUntil(f\"{year}-{month}-28\")\\\n",
    "            .setMaxTweets(1000)\n",
    "\n",
    "        # scraping tweets\n",
    "        tweets = got.manager.TweetManager.getTweets(tweetCriteria)\n",
    "\n",
    "        # adding all tweets to list\n",
    "        all_tweets.append(tweets)\n",
    "        \n",
    "        count = count + 1\n",
    "        print(\"-\"*15)\n",
    "        print(f\"Number of Tweets collected in {month}/{year}: {len(tweets)}\")\n",
    "        print(\"-\"*15)\n",
    "\n",
    "        # delay three seconds in between calls\n",
    "        time.sleep(3)\n",
    "        \n",
    "    return all_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "Number of Tweets collected in 1/2019: 1000\n",
      "---------------\n",
      "---------------\n",
      "Number of Tweets collected in 2/2019: 1000\n",
      "---------------\n",
      "---------------\n",
      "Number of Tweets collected in 3/2019: 1000\n",
      "---------------\n",
      "---------------\n",
      "Number of Tweets collected in 4/2019: 1000\n",
      "---------------\n",
      "---------------\n",
      "Number of Tweets collected in 5/2019: 1000\n",
      "---------------\n",
      "---------------\n",
      "Number of Tweets collected in 6/2019: 1000\n",
      "---------------\n",
      "---------------\n",
      "Number of Tweets collected in 7/2019: 1000\n",
      "---------------\n",
      "---------------\n",
      "Number of Tweets collected in 8/2019: 1000\n",
      "---------------\n",
      "---------------\n",
      "Number of Tweets collected in 9/2019: 1000\n",
      "---------------\n",
      "---------------\n",
      "Number of Tweets collected in 10/2019: 1000\n",
      "---------------\n",
      "---------------\n",
      "Number of Tweets collected in 11/2019: 1000\n",
      "---------------\n",
      "---------------\n",
      "Number of Tweets collected in 12/2019: 1000\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "restaurant = get_tweets(\"cheesecake factory\", \"2019\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding to MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet_bundle in restaurant:\n",
    "    for tweet in tweet_bundle:\n",
    "        # extracting information\n",
    "        date = tweet.date\n",
    "        tweet_text = tweet.text\n",
    "        num_favorites = tweet.favorites\n",
    "        num_replies = tweet.replies\n",
    "        num_retweets = tweet.retweets\n",
    "        author_username = tweet.username\n",
    "        tweet_id = tweet.id\n",
    "\n",
    "        # cleaning tweet text \n",
    "        clean_tokenized_tweet = clean_data(word_tokenize(tweet_text))\n",
    "\n",
    "        # creating dictionary object\n",
    "        post = {\n",
    "            \"search_term\": search_term,\n",
    "            \"date_posted\": date,\n",
    "            \"original_tweet\": tweet_text,\n",
    "            \"clean_tokenized_tweet\": clean_tokenized_tweet,\n",
    "            \"num_of_favorites\": num_favorites,\n",
    "            \"num_of_replies\": num_replies,\n",
    "            \"num_retweets\": num_retweets,\n",
    "            \"username\": author_username,\n",
    "            \"tweet_id\": tweet_id\n",
    "        }\n",
    "\n",
    "        # inserting object into database\n",
    "        db.restaurants.insert_one(post)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
