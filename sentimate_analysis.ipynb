{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter Sentiment Analysis\n",
    "#### References:\n",
    "* Training the NaiveBayesClassifier - https://www.digitalocean.com/community/tutorials/how-to-perform-sentiment-analysis-in-python-3-using-the-natural-language-toolkit-nltk#step-1-%E2%80%94-installing-nltk-and-downloading-the-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What I would do to improve analysis:\n",
    "* Include intent analysis - find the user's intention behind the tweet\n",
    "* Analyze larger tweet datasets\n",
    "* Lemmatize words like \"Hi\" and \"Hiiiii\"\n",
    "* Detect sarcasm in tweet\n",
    "* Sentiment analysis API service: https://www.paralleldots.com/sentiment-analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the NaiveBayesClassifier algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import twitter_samples\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer  #lemmatize text (running=run)\n",
    "from nltk.tag import pos_tag  #assign positional tags to tokens (noun, verb, adjective)\n",
    "import re, string  #filter regular expressions\n",
    "from nltk import NaiveBayesClassifier  #classifier algorithm\n",
    "from nltk import classify  #classify tweets as positive or negative\n",
    "from nltk import FreqDist  #determine the most frequent words in analysis\n",
    "import random\n",
    "from config import api_key, secret_api_key, access_token, secret_access_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['negative_tweets.json', 'positive_tweets.json', 'tweets.20150430-223406.json']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# viewing json files inside twitter_samples data\n",
    "twitter_samples.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove undesired text from tokens (puncuation, numbers & symbols, stop words)\n",
    "def clean_data(tweet_tokens, stop_words = ()):\n",
    "\n",
    "    cleaned_tokens = []\n",
    "\n",
    "    for token, tag in pos_tag(tweet_tokens):\n",
    "        # removing unwanted symbols and patterns from tokens using regular expressions\n",
    "        token = re.sub(\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\",\"\", token)\n",
    "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
    "        \n",
    "        # assigning new pos tags for WordNetLemmatizer() function\n",
    "        if tag.startswith(\"NN\"):\n",
    "            pos = \"n\"\n",
    "        elif tag.startswith(\"VB\"):\n",
    "            pos = \"v\"\n",
    "        else:\n",
    "            pos = \"a\"\n",
    "            \n",
    "        # lemmatizing tokens (running=run)\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        token = lemmatizer.lemmatize(token, pos)\n",
    "        \n",
    "        # dropping puncuation and stop words\n",
    "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
    "            cleaned_tokens.append(token.lower())\n",
    "    return cleaned_tokens\n",
    "            \n",
    "# covert list to dictionary with keys=tokens and values=true\n",
    "def get_tweets_for_model(cleaned_tokens_list):\n",
    "    for tweet_tokens in cleaned_tokens_list:\n",
    "        yield dict([token, True] for token in tweet_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizing, cleaning and assembling the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing positive/negative tweets\n",
    "positive_tokens = twitter_samples.tokenized(\"positive_tweets.json\")\n",
    "negative_tokens = twitter_samples.tokenized(\"negative_tweets.json\")\n",
    "\n",
    "# cleaning and prepping tokens for analysis\n",
    "positive_cleaned_tokens_list = []\n",
    "negative_cleaned_tokens_list = []\n",
    "for tokens in positive_tokens:\n",
    "    positive_cleaned_tokens_list.append(clean_data(tokens, stop_words))\n",
    "for tokens in negative_tokens:\n",
    "    negative_cleaned_tokens_list.append(clean_data(tokens, stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting lists of tokens to dictionaries with keys=tokens and values=True\n",
    "positive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)\n",
    "negative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)\n",
    "\n",
    "# creating dataset with assigned positive and negative setiment values\n",
    "positive_dataset = [(tweet_dict, \"Positive\") for tweet_dict in positive_tokens_for_model]\n",
    "negative_dataset = [(tweet_dict, \"Negative\") for tweet_dict in negative_tokens_for_model]\n",
    "\n",
    "# combining and shuffling positive and negative datasets \n",
    "dataset = positive_dataset + negative_dataset\n",
    "random.shuffle(dataset)\n",
    "\n",
    "# splitting dataset into train/test data\n",
    "train_data = dataset[:7000]\n",
    "test_data = dataset[7000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and testing the NaiveBayesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.9973333333333333\n",
      "Most Informative Features\n",
      "                      :) = True           Positi : Negati =    984.0 : 1.0\n",
      "                     sad = True           Negati : Positi =     56.5 : 1.0\n",
      "                followed = True           Negati : Positi =     23.1 : 1.0\n",
      "                follower = True           Positi : Negati =     22.1 : 1.0\n",
      "                     bam = True           Positi : Negati =     20.3 : 1.0\n",
      "                  friday = True           Positi : Negati =     16.1 : 1.0\n",
      "               community = True           Positi : Negati =     15.6 : 1.0\n",
      "                      aw = True           Negati : Positi =     14.4 : 1.0\n",
      "              appreciate = True           Positi : Negati =     14.3 : 1.0\n",
      "                   didnt = True           Negati : Positi =     13.7 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# training the NaiveBayesClassifier algorithm\n",
    "classifier = NaiveBayesClassifier.train(train_data)\n",
    "# assessing the accuracy of the algorithm\n",
    "print(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n",
    "# displaying the most informative words\n",
    "print(classifier.show_most_informative_features(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test the NaiveBayesClassifier on custom text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Positive'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "custom_tweet = \"That chicken alfredo was WOW!!\"\n",
    "\n",
    "custom_tokens = clean_data(word_tokenize(custom_tweet))\n",
    "result = classifier.classify(dict([token, True] for token in custom_tokens))\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Live Twitter Sentiment Analysis\n",
    "* Returns a 'sentiment score' calculated by the NaiveBayesClassifier for any topic provided by the user\n",
    "* 100 real-time tweets posted within the last seven days are analyzed to determine positive or negative sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the name of the topic to analyze for sentimate :cheesecake factory\n"
     ]
    }
   ],
   "source": [
    "# enable twitter api authorization\n",
    "auth = tweepy.OAuthHandler(api_key, secret_api_key)\n",
    "auth.set_access_token(access_token, secret_access_token)\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "# ask user for name of sentimate analysis subject\n",
    "user_input = input(\"Enter the name of the topic to analyze for sentimate :\")\n",
    "if user_input == \"\":\n",
    "    user_input = input(\"Enter the name of the topic to analyze for sentimate :\")\n",
    "    if user_input == \"\":\n",
    "        exit()\n",
    "\n",
    "# request the twitter api\n",
    "response = api.search(q=user_input,count=100,tweet_mode=\"extended\",lang=\"en\",result_type=\"mixed\") #result_type=mixed, recent, popular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive results: 74\n",
      "Negative results: 26\n"
     ]
    }
   ],
   "source": [
    "# tally positive and negative results based on the NaiveBayesClassifier algorithm\n",
    "positive_result=0\n",
    "negative_result=0\n",
    "positive_tweets = []\n",
    "negative_tweets = []\n",
    "for tweet in response:\n",
    "    tweet_text = tweet.full_text\n",
    "    tweet_tokens = clean_data(word_tokenize(tweet_text))\n",
    "    result = classifier.classify(dict([token, True] for token in tweet_tokens))\n",
    "    if result == \"Negative\":\n",
    "        negative_result += 1\n",
    "        negative_tweets.append(tweet.full_text)\n",
    "    elif result == \"Positive\":\n",
    "        positive_result += 1\n",
    "        positive_tweets.append(tweet.full_text)\n",
    "print(f\"Positive results: {positive_result}\")\n",
    "print(f\"Negative results: {negative_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Cheesecake Factory' scores a 3.7 out of 5.\n",
      "\n",
      "Positive Tweet Example: '@PCRCEPTlON i’m goin to the cheesecake factory for my birthday in 13 days ♡'\n",
      "\n",
      "Negative Tweet Example: 'I refer to The Cheesecake Factory as “Cheesecake” like that’s all ain’t nobody got time to be saying all them words 😂'\n"
     ]
    }
   ],
   "source": [
    "# calculate sentiment score\n",
    "sentiment_score = round((positive_result/100) * 5,2)\n",
    "print(f\"\"\"'{user_input.title()}' scores a {sentiment_score} out of 5.\n",
    "\"\"\")\n",
    "print(f\"\"\"Positive Tweet Example: '{random.choice(positive_tweets)}'\n",
    "\"\"\")\n",
    "print(f\"Negative Tweet Example: '{random.choice(negative_tweets)}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
